SEO Proof & Hardening — Final Test Pass (STRICT SCOPE)

Scope: Tests only. Do not change layout, styles, tool logic, or routes.
Goal: Verify every page has correct SEO tags, no duplicates, correct lengths, working OG images, valid sitemap/robots, and 200 responses.

1) Run a local, file‑based audit across all pages

Create scripts/seo-audit.mjs and run it. It must only read files under client/ and print a report.

# from repo root
mkdir -p scripts
printf '%s\n' "$(cat <<'EOF'
import { readFileSync, readdirSync } from 'fs';
import { join, extname } from 'path';

const ROOT = 'client';
const PAGES = [];
function crawl(dir) {
  for (const f of readdirSync(dir, { withFileTypes: true })) {
    const p = join(dir, f.name);
    if (f.isDirectory()) crawl(p);
    else if (extname(p).toLowerCase() === '.html') PAGES.push(p);
  }
}
crawl(ROOT);

function extract(str, re) { const m = str.match(re); return m ? m[1].trim() : ''; }

const results = [];
for (const page of PAGES) {
  const html = readFileSync(page, 'utf8');

  const title = extract(html, /<title>([\s\S]*?)<\/title>/i);
  const desc  = extract(html, /<meta\s+name=["']description["']\s+content=["']([\s\S]*?)["'][^>]*>/i);
  const canon = extract(html, /<link\s+rel=["']canonical["']\s+href=["']([\s\S]*?)["'][^>]*>/i);
  const h1cnt = (html.match(/<h1[\s>]/gi) || []).length;

  const ogt   = extract(html, /<meta\s+property=["']og:title["']\s+content=["']([\s\S]*?)["'][^>]*>/i);
  const ogd   = extract(html, /<meta\s+property=["']og:description["']\s+content=["']([\s\S]*?)["'][^>]*>/i);
  const ogi   = extract(html, /<meta\s+property=["']og:image["']\s+content=["']([\s\S]*?)["'][^>]*>/i);
  const twc   = extract(html, /<meta\s+name=["']twitter:card["']\s+content=["']([\s\S]*?)["'][^>]*>/i);

  results.push({
    page,
    title,
    titleLen: title.length,
    desc,
    descLen: desc.length,
    canonical: canon,
    h1Count: h1cnt,
    ogTitle: ogt,
    ogDesc: ogd,
    ogImage: ogi,
    twitterCard: twc
  });
}

// Duplicate detection
function dupes(key) {
  const map = new Map();
  for (const r of results) {
    const k = (r[key] || '').toLowerCase();
    if (!k) continue;
    map.set(k, (map.get(k) || []).concat(r.page));
  }
  return [...map.entries()].filter(([,v]) => v.length > 1);
}

const dupTitles = dupes('title');
const dupDescs  = dupes('desc');

// Basic heuristics
const issues = [];
for (const r of results) {
  if (!r.title) issues.push([r.page, 'MISSING_TITLE']);
  if (r.titleLen > 60) issues.push([r.page, `TITLE_TOO_LONG(${r.titleLen})`]);
  if (!r.desc) issues.push([r.page, 'MISSING_DESCRIPTION']);
  if (r.descLen < 80 || r.descLen > 170) issues.push([r.page, `DESC_LENGTH(${r.descLen})`]);
  if (!r.canonical) issues.push([r.page, 'MISSING_CANONICAL']);
  if (r.h1Count !== 1) issues.push([r.page, `H1_COUNT(${r.h1Count})`]);
  if (!r.ogTitle || !r.ogDesc || !r.ogImage) issues.push([r.page, 'MISSING_OG_TAGS']);
  if (!r.twitterCard) issues.push([r.page, 'MISSING_TWITTER_TAGS']);
}

console.log('=== SEO AUDIT SUMMARY ===');
console.log(`Pages scanned: ${results.length}`);
if (dupTitles.length) {
  console.log('\nDuplicate Titles:');
  for (const [t, pages] of dupTitles) console.log('-', t, '->', pages.join(', '));
}
if (dupDescs.length) {
  console.log('\nDuplicate Descriptions:');
  for (const [d, pages] of dupDescs) console.log('-', d, '->', pages.join(', '));
}
if (issues.length) {
  console.log('\nIssues:');
  for (const [p, i] of issues) console.log('-', p, ':', i);
} else {
  console.log('\nNo issues detected by static audit.');
}

console.log('\nDetail JSON:');
console.log(JSON.stringify(results, null, 2));
EOF
)" > scripts/seo-audit.mjs
node scripts/seo-audit.mjs


Deliverables: paste the audit console output. There should be no missing tags, no duplicates, desc length ~80–170, and exactly one <h1> per page.

2) Live server checks (200 responses + canonical correctness)

Run these and paste outputs:

# adjust if your dev server runs on a different port
ENDPOINTS=(
  "/" "/pdf-tools/pdf-compress.html" "/pdf-tools/pdf-merge.html" "/pdf-tools/pdf-split.html"
  "/image-tools/heic-to-jpg.html" "/image-tools/jpg-to-png.html" "/audio-video-tools/mp4-to-mp3.html"
  "/sitemap.xml" "/robots.txt"
)
for p in "${ENDPOINTS[@]}"; do
  code=$(curl -s -o /dev/null -w "%{http_code}" "http://localhost:5000$p")
  echo "$p -> $code"
done

# Spot-check that canonical is https://<prod-domain>/<route>
grep -oE '<link[^>]+rel="canonical"[^>]*>' client/**/*.html


All endpoints must return 200 and canonicals must point to your production domain.

3) OG image integrity

If you set per‑page OG images (e.g., /og/pdf-compress.png), confirm they exist and return 200:

IMAGES=(
  "/og/pdf-compress.png" "/og/pdf-merge.png" "/og/jpg-to-png.png"
)
for i in "${IMAGES[@]}"; do
  code=$(curl -s -o /dev/null -w "%{http_code}" "http://localhost:5000$i")
  echo "$i -> $code"
done


(If you used a shared image, test that one.)

4) Structured data (if added)

For any page with FAQ JSON‑LD, print the block to verify it exists:

grep -n "FAQPage" -n client/**/*.html || echo "No FAQ schema found."

5) No‑regression smoke test (navigation only)
# Just ensure pages load; do NOT change functionality.
for p in client/*.html client/*/*.html; do
  rel="/${p#client/}"
  code=$(curl -s -o /dev/null -w "%{http_code}" "http://localhost:5000$rel")
  echo "$rel -> $code"
done


All must be 200.

Acceptance Criteria

Static audit reports 0 issues and 0 duplicates.

All routes, sitemap, robots, and OG assets return HTTP 200.

Canonicals point to the correct production URLs.

No layout or tool behavior changes introduced.

If anything fails, report the specific file and line to fix—do not change unrelated files.